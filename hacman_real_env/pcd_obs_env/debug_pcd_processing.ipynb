{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Started camera 0Started camera 1\n",
      "\n",
      "Started camera 3\n",
      "Started camera 2\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pcd_obs_env import PCDObsEnv\n",
    "env = PCDObsEnv(\n",
    "    # camera_indices=[0,2],\n",
    "    camera_alignments=False) # set to False to start from scratch\n",
    "\n",
    "colors = [np.random.rand(3) for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Camera PCD Preprocessing\n",
    "\n",
    "Processing consists of the following steps:\n",
    "- random downsample to 1/4 of the points\n",
    "- voxel downsample\n",
    "- remove outliers\n",
    "- estimate normals\n",
    "- compute FPFH features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. open3d.cpu.pybind.utility.Vector3dVector()\n    2. open3d.cpu.pybind.utility.Vector3dVector(arg0: numpy.ndarray[float64])\n    3. open3d.cpu.pybind.utility.Vector3dVector(arg0: open3d.cpu.pybind.utility.Vector3dVector)\n    4. open3d.cpu.pybind.utility.Vector3dVector(arg0: iterable)\n\nInvoked with: PointCloud with 329287 points.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Obtain the point cloud\u001b[39;00m\n\u001b[1;32m     46\u001b[0m pcd \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_single_raw_pcd(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m pcd \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mPointCloud(\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVector3dVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m pcd\u001b[38;5;241m.\u001b[39mestimate_normals(search_param\u001b[38;5;241m=\u001b[39mo3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mKDTreeSearchParamHybrid(radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m))\n\u001b[1;32m     49\u001b[0m o3d\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mdraw_geometries([pcd])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. open3d.cpu.pybind.utility.Vector3dVector()\n    2. open3d.cpu.pybind.utility.Vector3dVector(arg0: numpy.ndarray[float64])\n    3. open3d.cpu.pybind.utility.Vector3dVector(arg0: open3d.cpu.pybind.utility.Vector3dVector)\n    4. open3d.cpu.pybind.utility.Vector3dVector(arg0: iterable)\n\nInvoked with: PointCloud with 329287 points."
     ]
    }
   ],
   "source": [
    "def display_inlier_outlier(cloud, ind):\n",
    "    # Compute normals to help visualize\n",
    "    # cloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "\n",
    "    inlier_cloud = cloud.select_by_index(ind)\n",
    "    inlier_cloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "    outlier_cloud = cloud.select_by_index(ind, invert=True)\n",
    "\n",
    "    print(\"Showing outliers (red) and inliers (gray): \")\n",
    "    outlier_cloud.paint_uniform_color([1, 0, 0])\n",
    "    inlier_cloud.paint_uniform_color([0.8, 0.8, 0.8])\n",
    "    o3d.visualization.draw_geometries([inlier_cloud, outlier_cloud])\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    print(\":: Random downsample to 1/4 size.\")\n",
    "    pcd_size = len(pcd.points)\n",
    "    pcd_down_mask = np.random.choice(pcd_size, int(pcd_size / 4))\n",
    "    pcd_down = pcd.select_by_index(pcd_down_mask)\n",
    "\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd_down.voxel_down_sample(voxel_size)\n",
    "    # o3d.visualization.draw_geometries([pcd_down])\n",
    "\n",
    "    radius = 0.02\n",
    "    print(\":: Remove radius outlier with radius %.3f.\" % radius)\n",
    "    nb_points = int(((radius / voxel_size) ** 2) * 0.9) \n",
    "    # cl, ind = pcd_down.remove_radius_outlier(nb_points=nb_points, radius=radius)\n",
    "    cl, ind = pcd_down.remove_statistical_outlier(nb_neighbors=32, std_ratio=0.3)\n",
    "    display_inlier_outlier(pcd_down, ind)\n",
    "    pcd_down = cl\n",
    "\n",
    "    radius_normal = voxel_size * 4\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "voxel_size = 0.005\n",
    "# Obtain the point cloud\n",
    "pcd = env.get_single_raw_pcd(0)\n",
    "pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd))\n",
    "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "print(f\"PCD shape: {len(pcd.points)}\")\n",
    "pcd_down, pcd_fpfh = preprocess_point_cloud(pcd, voxel_size)\n",
    "o3d.visualization.draw_geometries([pcd_down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cam 0. Capture takes 0.323s. Processing takes 0.039s.\n",
      "Cam 1. Capture takes 0.732s. Processing takes 0.032s.\n",
      "Cam 2. Capture takes 0.741s. Processing takes 0.035s.\n",
      "Cam 3. Capture takes 0.730s. Processing takes 0.033s.\n"
     ]
    }
   ],
   "source": [
    "# To test if the pcd looks good after preprocessing\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for cam_id in env.camera_indices:\n",
    "    processed_pcd = env.get_single_pcd(cam_id)\n",
    "    processed_pcd.paint_uniform_color(colors[cam_id])\n",
    "    pcd += processed_pcd\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning Multiple Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cam 0. Capture takes 0.313s. Processing takes 0.040s.\n",
      "Cam 1. Capture takes 0.322s. Processing takes 0.031s.\n",
      "Cam 2. Capture takes 0.326s. Processing takes 0.035s.\n",
      "Cam 3. Capture takes 0.319s. Processing takes 0.030s.\n",
      ":: Aligning camera 0 with target pcd\n",
      ":: Apply point-to-point ICP\n",
      "RegistrationResult with fitness=6.381103e-01, inlier_rmse=4.613434e-03, and correspondence_set size of 17735\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.99980266 -0.00900564 -0.01770706  0.01435733]\n",
      " [ 0.01032803  0.99704923  0.07606682 -0.03695451]\n",
      " [ 0.01696978 -0.07623469  0.99694548 -0.03527774]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      ":: Aligning camera 1 with target pcd\n",
      ":: Apply point-to-point ICP\n",
      "RegistrationResult with fitness=6.225879e-01, inlier_rmse=4.516204e-03, and correspondence_set size of 15325\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.99963911 -0.02295392 -0.01395622  0.01018804]\n",
      " [ 0.0234809   0.99896924  0.03884727  0.00202732]\n",
      " [ 0.01305013 -0.03916095  0.99914769 -0.02268798]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      ":: Aligning camera 2 with target pcd\n",
      ":: Apply point-to-point ICP\n",
      "RegistrationResult with fitness=1.000000e+00, inlier_rmse=0.000000e+00, and correspondence_set size of 29335\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      ":: Aligning camera 3 with target pcd\n",
      ":: Apply point-to-point ICP\n",
      "RegistrationResult with fitness=5.194732e-01, inlier_rmse=4.368601e-03, and correspondence_set size of 11991\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 9.99870246e-01 -1.61086139e-02 -5.26795796e-05  2.17642687e-02]\n",
      " [ 1.60998028e-02  9.99203979e-01  3.64993820e-02 -2.98741049e-03]\n",
      " [-5.35316805e-04 -3.64954942e-02  9.99333674e-01 -8.25635813e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "def compute_align_to_target(target_pcd, other_pcds, threshold=0.01, visualize=False):\n",
    "    \"\"\"\n",
    "    Compute alignments from other_pcds to base_pcd\n",
    "    \n",
    "    Input:\n",
    "        target_pcd: Open3D point cloud\n",
    "        other_pcds: dict of Open3D point clouds {cam_id: pcd}\n",
    "    Return:\n",
    "        dict of transforms {cam_id: transforms}\n",
    "    \"\"\"\n",
    "    transforms = {}\n",
    "    for cam_id, source in other_pcds.items():        \n",
    "        print(f\":: Aligning camera {cam_id} with target pcd\")\n",
    "        print(\":: Apply point-to-point ICP\")\n",
    "        trans_init = np.identity(4)\n",
    "        # reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        #     source, target, threshold, trans_init,\n",
    "        #     o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "        source.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "        target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "            source, target_pcd, threshold, trans_init,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "        print(reg_p2p)\n",
    "        print(\"Transformation is:\")\n",
    "        print(reg_p2p.transformation)\n",
    "        if visualize:\n",
    "            draw_registration_result(source, target_pcd, reg_p2p.transformation)\n",
    "\n",
    "        transforms[cam_id] = reg_p2p.transformation.copy()\n",
    "    \n",
    "    # For base camer, set to identity\n",
    "    transforms[base_cam_id] = np.identity(4)\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "def align_pcds(pcds, transforms):\n",
    "    \"\"\"\n",
    "    Align point clouds using transforms\n",
    "    \n",
    "    Input:\n",
    "        pcds: dict of Open3D point clouds {cam_id: pcd}\n",
    "        transforms: dict of transforms {cam_id: transforms}.\n",
    "    Return:\n",
    "        Open3D point cloud\n",
    "    \"\"\"\n",
    "    transformed_pcds = o3d.geometry.PointCloud()\n",
    "    for cam_id in env.camera_indices:\n",
    "        transformed_pcd = pcds[cam_id].transform(transforms[cam_id])\n",
    "        transformed_pcd.paint_uniform_color(colors[cam_id])\n",
    "        transformed_pcds += transformed_pcd\n",
    "    \n",
    "    return transformed_pcds\n",
    "\n",
    "\n",
    "base_cam_id = 2     # Set all other cameras to align with camera 0\n",
    "\n",
    "# Obtain individual point clouds\n",
    "pcds = {}\n",
    "for cam_id in env.camera_indices:\n",
    "    pcds[cam_id] = env.get_single_pcd(cam_id)\n",
    "\n",
    "# Compute alignments\n",
    "transforms = compute_align_to_target(\n",
    "    pcds[base_cam_id], pcds, threshold=0.01, visualize=False) # Set to True to visualize pairwise alignment\n",
    "\n",
    "# Transform all point clouds\n",
    "camera_aligned_pcds = align_pcds(pcds, transforms)\n",
    "o3d.visualization.draw_geometries([camera_aligned_pcds])\n",
    "\n",
    "# Remove color\n",
    "camera_aligned_pcds_ = o3d.geometry.PointCloud(camera_aligned_pcds.points)\n",
    "camera_aligned_pcds_.normals = camera_aligned_pcds.normals \n",
    "o3d.visualization.draw_geometries([camera_aligned_pcds_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save the results\"\"\"\n",
    "import os, json\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "save_dir = os.path.join(\"calibration\", \"finetune_results\")\n",
    "\n",
    "# Save the transform matrices as npz\n",
    "save_path = os.path.join(save_dir, \"camera_alignments.npz\")\n",
    "save_content = {\n",
    "    str(cam_id): transform for cam_id, transform in transforms.items()\n",
    "} # Convert cam_id to string to save as npz\n",
    "np.savez(save_path, **save_content)\n",
    "\n",
    "# Also save the humann readable transforms as xyz, quat into json\n",
    "save_content = {}\n",
    "for cam_id, transform in transforms.items():\n",
    "    quat = Rotation.from_matrix(transform[:3, :3]).as_quat()\n",
    "    save_content[cam_id] = {\n",
    "        \"xyz\": transform[:3, 3].tolist(),\n",
    "        \"quaternion\": quat.tolist()\n",
    "    }\n",
    "save_path = os.path.join(save_dir, \"camera_alignments.json\")\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(save_content, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning Base PCD to Ground Truth Plane (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Aligning camera combined with target pcd\n",
      ":: Apply point-to-point ICP\n",
      "RegistrationResult with fitness=6.228153e-01, inlier_rmse=3.967222e-03, and correspondence_set size of 48001\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 9.99999017e-01 -4.28847140e-07  1.40235593e-03  5.48236772e-06]\n",
      " [-4.20704468e-06  9.99994536e-01  3.30578368e-03 -7.09808724e-06]\n",
      " [-1.40234968e-03 -3.30578633e-03  9.99993553e-01 -6.12885909e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "gt_plane = np.meshgrid(np.arange(0.1, 0.85, 0.005), np.arange(-0.85, 0.25, 0.005))\n",
    "gt_plane = np.vstack([gt_plane[0].flatten(), gt_plane[1].flatten(), np.zeros_like(gt_plane[0].flatten())]).T\n",
    "gt_plane_pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(gt_plane))\n",
    "gt_plane_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "\n",
    "# Obtain individual point clouds\n",
    "pcds = {}\n",
    "for cam_id in env.camera_indices:\n",
    "    pcds[cam_id] = env.get_single_pcd(cam_id)\n",
    "# Transform all point clouds (using previous alignment)\n",
    "camera_aligned_pcds = align_pcds(pcds, transforms)\n",
    "\n",
    "# Visualize point clouds\n",
    "obs_pcd = copy.deepcopy(camera_aligned_pcds)\n",
    "draw_registration_result(obs_pcd, gt_plane_pcd, np.identity(4))\n",
    "\n",
    "# Compute alignments\n",
    "pcds = {\"combined\": obs_pcd}\n",
    "gt_transforms = compute_align_to_target(\n",
    "    gt_plane_pcd, pcds, threshold=0.01, visualize=False) # Set to True to visualize pairwise alignment\n",
    "\n",
    "# We only need the z transform and the rotation\n",
    "base_transform = gt_transforms[\"combined\"]\n",
    "base_transform[0:2, 3] = 0\n",
    "\n",
    "# Apply base transform to all other transforms\n",
    "transformed_pcd = obs_pcd.transform(base_transform)\n",
    "draw_registration_result(obs_pcd, gt_plane_pcd, base_transform)\n",
    "# transformed_pcd = obs_pcd.transform(base_transform)\n",
    "# transformed_pcd.paint_uniform_color([1, 0, 0])\n",
    "# obs_pcd.paint_uniform_color([0, 0, 1])\n",
    "# o3d.visualization.draw_geometries([\n",
    "#     # transformed_pcd,\n",
    "#     obs_pcd,\n",
    "#     gt_plane_pcd\n",
    "#     ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacman-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
